wordmaker 词语生成工具
=========
通过词语组成的规律，自动从大文本当中学习得到文本当中的词语，而不再需要其它额外的信息。

很多分词库等都需要字典库，特别在一些专业的领域，需要得到很多的专业相关词语。而人工标注字典需要花很大的时间，所以希望有一个工具能够自动从文本中训练得到词语。分析某类人的用词特点，也可以有所应用。

重点说明
========
* 大家通过邮件与我讨论问题我一般都会回复，但请尽可能在项目中通过open issue的方式提交问题。这样不仅会让我更及时看到问题，还能给项目的其它人带来好处。相信方便你也能方便其它后来的朋友。若您是open心态的人，应该明白这样的好处。
* 项目已经搬迁至[avplayer旗下](https://github.com/avplayer/wordmaker)， 有问题请在此项目下open issue。
* [avboost](http://avboost.com/) 是一个非常好的C++社区，虽然我与上面的各路版主不熟悉，也没有时间在上面发帖讨论问题，但是相信他们各种都是非常open非常牛的精英，所以把项目移到他们名下会让项目变得更好。

代码实现
========
最初尝试实现了一个[简单版本](https://github.com/jannson/yaha/blob/master/extra/segword.cpp)，但基于单线程，运行速度慢，并且还消耗巨大的内存。最近尝试接触c++ 11，并使用了类似map/reduce思想，所以决定拿这个项目练练手，也希望能有人多交流。思想如下：

* 基于节约内存的 Trie树结构 （Double Array Trie & MARISA）
* 用多个线程独立计算各个文本块的词的信息，再按词的顺序分段合并，再计算各个段的字可能组成词的概率，左右熵，得到词语输出。 因为都分成独立的块处理，所以使用多线程非常方便。这种思想应该可以应用在大部分的文本处理工作中，若有更好的办法欢迎交流~
* src/wordmaker.cpp 将在内存当中完成所有计算，并只依赖于cedar.h文件。能够很快处理20M左右的文本。
* src/hugemaker.cpp 
为了节约内存，在计算、合并过程中将保存部分中间文件。可以处理更大的文本(50M+)。marisa:Trie支持mmap将trie文件mmap到内存当中，经过修改完全可以处理更巨大的文本（当时用的时间也会更久）。
* 默认启用4个线程，可以修改代码的宏，也可以自己加shell命令控制。
* 为了代码的简单，只支持gbk编码。（没太多检查，其它编码格式的输入将可能产生段错误，欢迎帮优化）
* 在linux与cygwin下编码成功，visual studio下应该也没问题吧。
* 因工作关系用C比C++时间多得多，希望使用代码的人看到代码有任何不符合现在c++观点的作法及时指出，以做交流！

编译与使用
==========
mkdir build

cd build

cmake28 ..

make

./bin/wordmaker input.txt output.txt  或者

./bin/hugemaker input.txt output.txt

linux下可以使用：

iconv -f "gbk" -t "utf-8//IGNORE" < infile > outfile

进行编码转换。windows下当然可以使用notepad++了，转换成ANSI。

测试语料
========
sogou的新闻语料，把各个文本合成在一起总共50M：http://pan.baidu.com/s/1mgoIPxY
里面还有莫言的文集当做输入语料，方法大家测试代码。

这里是我运行的[表现结果](https://github.com/jannson/wordmaker/tree/master/tests)


TODO
========
* 算法优化：结果里偶尔出现 “鼓舞了”、“默认了”、“鲜红的”等错误词，虽然算法里已经较好的处理了这种情况，但感觉应该有更好的办法，比如：给类似“的”、“了”、“在”等出现频率特别高的字更低的组词权重，而提高其它一些情况的词的权重，可能输出结果会更好。

* 找到更多的语料，结合本工具，并实现一些办法，从这些语料当中分析得到更多的有趣信息。比如说网络流行词，某行业的专用词等。
